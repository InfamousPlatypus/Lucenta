# Lucenta Configuration Sample
# Copy this file to .env and fill in your values

# ============================================
# Local LLM Provider Configuration
# ============================================
# Options: ollama, llamacpp, openai, anthropic, gemini
LOCAL_PROVIDER=ollama
DEFAULT_MODEL_NAME=llama3.1:latest
DEFAULT_MODEL_BASE_URL=http://localhost:11434

# llama.cpp Configuration (if using llamacpp)
# LLAMACPP_BINARY=llama-cli
# LLAMACPP_MODEL_PATH=/path/to/model.gguf

# ============================================
# External API Fallback (Optional)
# ============================================
# Used when local resources are constrained
# OPENAI_API_KEY=sk-your-key-here
# Or use other providers:
# ANTHROPIC_API_KEY=your-key-here
# GEMINI_API_KEY=your-key-here

# ============================================
# Communication Gateways (Optional)
# ============================================
# Telegram Bot
# TELEGRAM_BOT_TOKEN=your-telegram-bot-token

# Email Gateway
# EMAIL_USER=your-email@example.com
# EMAIL_PASS=your-email-password
# EMAIL_IMAP_SERVER=imap.gmail.com
# EMAIL_SMTP_SERVER=smtp.gmail.com

# ============================================
# MCP Server Configuration
# ============================================
# Path to your MCP servers directory
MCP_SERVERS_PATH=C:\Users\mark\public-apis\mcp-servers

# Container runtime: false=Docker (default), true=Podman
USE_PODMAN=false

# NASA API Key (optional, DEMO_KEY works for testing)
# NASA_API_KEY=DEMO_KEY

# Hugging Face API Token (optional, for private datasets/models)
# HUGGING_FACE_API_TOKEN=hf_your_token_here
